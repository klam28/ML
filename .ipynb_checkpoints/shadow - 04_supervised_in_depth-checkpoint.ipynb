{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ogrisel/sklearn_pycon2014/blob/master/notebooks/04_supervised_in_depth.ipynb\n",
    "04_supervised_in_depth.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning In-Depth: SVMs and Random Forests Machine Learning Algorithms\n",
    "- Understand how decision trees work\n",
    "- Understand how multiple decision trees are combined into Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "- Support Vector Machines (SVMs) are a powerful supervised learning algorithm used for classification or for regression. \n",
    "    - SVMs are a discriminative classifier: that is, they draw a boundary between clusters of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, target = make_blobs(n_samples = 100, centers = 2,\n",
    "                  random_state = 2, cluster_std = 0.75) # cluster_std ~ Determines How Far Apart The Points Are\n",
    "plt.scatter(X[:, 0], X[:, 1], c = target, s = 15); # s = Size Of Points, c = Color Of Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking at Shape of X data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit Support Vector Machine Classifier To The Points\n",
    "from sklearn.svm import SVC \n",
    "clf = SVC(kernel = 'linear')\n",
    "clf.fit(X, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(clf):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function([xi, yj])\n",
    "    return plt.contour(X, Y, P, colors='k',\n",
    "                       levels=[-1, 0, 1],\n",
    "                       linestyles=['--', '-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = X.reshape(1,-1) # (1 Row, max_columns)\n",
    "# X[0, :5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = X[:,0]\n",
    "B = X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = A[:, np.newaxis]\n",
    "B = B[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(A, B, c = target, s = 15)\n",
    "\n",
    "plot_svc_decision_function(clf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,n_features = 2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forests\n",
    "Random Forests Are An Example Of Ensemble Learner (Supervised Learning) Built On Decision Trees\n",
    "\n",
    "# Decision Trees\n",
    "\n",
    "Decision Trees Encodes A Series Of Binary Choices Breaking Off Of A Single Question As Binary To Classify Things. The Binary Splitting Of Questions Is The Essence Of A Decision Tree. Example: Creating A Guide To Identify An Animal Found In Nature, We'D Ask The Following Questions:\n",
    "    - Is The Animal Big Or Small?\n",
    "        - If Big: Does The Animal Have Wings Or No Wings?\n",
    "            - If Wings: Does It Live In A Cave?\n",
    "            - If No Wings: Does It Hunt At Night?\n",
    "        - If Small: Does The Animal Have Fur:\n",
    "            - If Fur: Does It Hop?\n",
    "            - If No Fur: Does It Breed Eggs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, target = make_blobs(n_samples=1000, centers=7,\n",
    "                  random_state=3, cluster_std=0.75)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=target, s=20)\n",
    "\"\"\"KL: Creating Plot With 7 Clusters, High Standard Deviation Sets Them Farther Apart The \\\n",
    "Higher The Standard Deviation. s Parameter Sets The Size Of The Points\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_estimator(estimator, observations, target):\n",
    "    estimator.fit(observations, target)\n",
    "    x_min, x_max = observations[:, 0].min() - .1, observations[:, 0].max() + .1\n",
    "    y_min, y_max = observations[:, 1].min() - .1, observations[:, 1].max() + .1\n",
    "    print(x_min, x_max)\n",
    "    print(y_min, y_max)\n",
    "    \n",
    "    # Set xx And yy To The Minimum And Maximum Values Of X_Min, X_Max And Y_Min, Y_Max \n",
    "    # From Observations Values\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50), # xx linspace\n",
    "                         np.linspace(y_min, y_max, 50)) # yy linspace\n",
    "    \n",
    "    print(xx)\n",
    "    print(yy)\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, alpha=0.3)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(observations[:, 0], observations[:, 1], c=target, s=25)\n",
    "    plt.axis('tight')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=10)\n",
    "plot_estimator(classifier, X, target)\n",
    "# x_min, x_max = observations[:, 0].min() - .1, observations[:, 0].max() + .1\n",
    "# y_min, y_max = observations[:, 1].min() - .1, observations[:, 1].max() + .1\n",
    "\n",
    "# # Set xx And yy To The Minimum And Maximum Values Of X_Min, X_Max And Y_Min, Y_Max \n",
    "# # From Observations Values\n",
    "# xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50), # x \n",
    "#                          np.linspace(y_min, y_max, 50)) # y\n",
    "\n",
    "#     # Put the result into a color plot\n",
    "# print(x_min)\n",
    "# print(y_min)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
